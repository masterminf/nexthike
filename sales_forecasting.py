# -*- coding: utf-8 -*-
"""Sales forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IDb02Dcf0TNpJkwXCKkGg3y_4wi51gr8
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

store=pd.read_csv("store.csv")
train =pd.read_csv("train.csv")
test =pd.read_csv("test.csv")

store.head()

train.head()

# as there are different columns in store and train dataset we have to merge to get main datset
df=train.merge(store,on="Store",how="left")

df.head()

"""# **Task 1 - Exploration of customer purchasing behaviour**

task 1.1 - we have to check promotion distribution between train and test groups
"""

# Percentage of promo in training
train_promo_dist = train['Promo'].value_counts(normalize=True) * 100

# Percentage of promo in test
test_promo_dist = test['Promo'].value_counts(normalize=True) * 100

print("Training set Promo distribution:\n", train_promo_dist)
print("\nTest set Promo distribution:\n", test_promo_dist)

"""task 1.2 calculate sales before and after holiday"""

# first we have conver date column in datetime format
df["date"] = pd.to_datetime(df["Date"], errors='coerce')

#first we have to check for open days
df = df[df["Open"]==1].copy()

# we have to make sure stateholiday is string
df["StateHoliday"] = df["StateHoliday"].astype(str)

# now create holidayperiod as new column
df["HolidayPeriod"] = 'No Holiday'

# we have to identify sales between holdidays
df.loc[df['StateHoliday'] != '0', 'HolidayPeriod'] = 'During Holiday'

#  now we have to Identify holiday dates and ensure datetime
holyday_dates = pd.to_datetime(df.loc[df['StateHoliday'] != '0', 'Date'].unique())

# now go for before and after holydays
for hdate in holyday_dates:
    before = hdate - pd.Timedelta(days=1)
    after = hdate + pd.Timedelta(days=1)
    df.loc[df['Date'] == before, 'HolidayPeriod'] = 'Before Holiday'
    df.loc[df['Date'] == after, 'HolidayPeriod'] = 'After Holiday'

# Calculate average sales for each like during,before and after hilyday
sales_for_each_period = df.groupby('HolidayPeriod')['Sales'].mean().reset_index()

print(sales_for_each_period)

# even we can plot comaprison for each period
plt.bar(sales_for_each_period['HolidayPeriod'], sales_for_each_period['Sales'], color=['orange', 'green', 'red', 'blue'])
plt.title("Average Sales Before, During, and After Holidays")
plt.ylabel("Average Sales")
plt.xlabel("Period")
plt.show()

"""
task 1.3 Find out any seasonal (Christmas, Easter etc) purchase behaviours
"""

#  first Extract month of year
train['Date'] = pd.to_datetime(train['Date'], errors='coerce')

df = train[train['Open'] == 1].copy()   # filter only where Open == 1
df['Month'] = df['Date'].dt.month

# now extract day of year
df['DayOfYear'] = df['Date'].dt.dayofyear

# now we can calculate Average sales by month
monthly_sale = df.groupby('Month')['Sales'].mean().reset_index()

# Plot monthly sales pattern
plt.figure(figsize=(10,5))
plt.plot(monthly_sale['Month'], monthly_sale['Sales'], marker='o')
plt.xticks(range(1,13))
plt.title("Average Monthly Sales Pattern")
plt.xlabel("Month")
plt.ylabel("Average Sales")
plt.grid(True)
plt.show()

# now we can calculte season wise sales behaviour
# for Christmas
Christmas_sales = df[df['Month'] == 12].groupby('DayOfYear')['Sales'].mean()
plt.figure(figsize=(10,5))
plt.plot(Christmas_sales.index, Christmas_sales.values, marker='o', color='red')
plt.title("Christmas_sales (Christmas Period)")
plt.xlabel("Day of Year")
plt.ylabel("Average Sales")
plt.grid(True)
plt.show()

# for ester
Easter_sales = df[df['Month'] == 4].groupby('DayOfYear')['Sales'].mean()
plt.figure(figsize=(8,4))
plt.plot(Easter_sales.index, Easter_sales.values, marker='o', color='green')
plt.title("Easter_sales(Easter Period)")
plt.xlabel("Day of Year")
plt.ylabel("Average Sales")
plt.grid(True)
plt.show()

"""Task 1.3 What can you say about the correlation between sales and number of customers?

"""

# find out correlation
correlation = df['Sales'].corr(df['Customers'])
print(correlation)


# from the result we can say that the correlation bet sales and customer is very strong positive relation.

"""task 1.4 How does promo affect sales? Are the promos attracting more customers? How does it affect already existing customers?

"""

# first we will find out affest of promo on sales
affest_promo_sales = df.groupby('Promo')['Sales'].mean()
print(affest_promo_sales)

# if the sale is higher we can say promo is working positively.

# now we will find out affect of promo on customer
affest_promo_customers = df.groupby('Promo')['Customers'].mean()
print(affest_promo_customers)

# if the number of customer increasing during promo we can say that promo attracting new shoppers in the shop.

# affect of promo on existing customer
df['Sales_per_Customer'] = df['Sales'] / df['Customers']
effect_on_existing = df.groupby('Promo')['Sales_per_Customer'].mean()
print(effect_on_existing)

"""Trends of customer behavior during store open and closing times

"""

# first find out day_of_week trend
days_of_week = df.groupby('DayOfWeek')[['Sales','Customers']].mean().reset_index()
print(days_of_week)

# store before clossing and store after opening
df['PrevOpen'] = df['Open'].shift(1)
closing_effect = df.groupby(['PrevOpen','Open'])[['Sales','Customers']].mean()
print(closing_effect)

"""Which stores are opened on all weekdays? How does that affect their sales on weekends?

"""

# here there are 2 scenario  1.-open on weekdays  1.-open on weekends
# first we will go with 1 case
# Filter only open records
open_days = df[df['Open'] == 1]

# Check which weekdays each store is open
store_weekday_open = open_days.groupby(['Store','DayOfWeek']).size().unstack(fill_value=0)

# Stores open on all weekdays (Mon=1 ... Fri=5)
open_all_weekdays = store_weekday_open[(store_weekday_open.loc[:,1:5] > 0).all(axis=1)].index

# now we will go with 2nd case
# Mark if store is open all weekdays
df['OpenAllWeekdays'] = df['Store'].isin(open_all_weekdays)

# Weekend sales only
weekend_sales = df[df['DayOfWeek'].isin([6,7])]

# Compare average weekend sales
comparison = weekend_sales.groupby('OpenAllWeekdays')['Sales'].mean()
print(comparison)

df.head()

"""How does the distance to the next competitor affect sales? What if the store and
its competitors all happen to be in city centres, does the distance matter in that
case?
"""

# Merge train and store data to get CompetitionDistance
merged_df = train.merge(store,on="Store",how="left")
sns.scatterplot(x="CompetitionDistance", y="Sales", data=merged_df, alpha=0.3)
plt.xlim(0, 10000)  # zoom in (most stores are within 10km)
plt.title("Effect of Competition Distance on Sales")
plt.show()

# from the above graph we can say that distance of store matter more in rural/village than city

# how new competetor affest sales
merged_df = train.merge(store,on="Store",how="left")
stores_with_new_competitor = (
    merged_df.groupby("Store")["CompetitionDistance"]
         .apply(lambda x: x.isna().any() and x.notna().any())
)

affected_stores = stores_with_new_competitor[stores_with_new_competitor].index
print("Stores with new competitors:", affected_stores.tolist())

"""### **Task 2 - Prediction of store sales**"""

# feature enginnering on date column
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Quarter'] = df['Date'].dt.quarter

# now Weekend flag
df['IsWeekend'] = df['DayOfWeek'].isin([5,6]).astype(int)

# go for Beginning/Mid/End of Month
df['MonthPeriod'] = pd.cut(df['Day'],
                           bins=[0,10,20,31],
                           labels=['Beginning','Mid','End'])

# Days to next holiday & days after last holiday
df = df.sort_values(by=['Store','Date'])
df['HolidayFlag'] = ((df['StateHoliday'] != '0') & (df['StateHoliday'] != 0)).astype(int)

# we will count day since last holiday
df['DaysSinceHoliday'] = df.groupby('Store')['HolidayFlag'].cumsum()

# now we will go for EDA and first will handle categorical columns
from sklearn.preprocessing import StandardScaler, LabelEncoder,OneHotEncoder
categoricals = ['StoreType','Assortment','StateHoliday','MonthPeriod']
for col in categoricals:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))

# check for null value
df.isnull().sum()

df.fillna(0, inplace=True)

df.isnull().sum()

"""2.2 Building models with sklearn pipelines

"""

# importing all neccessary libearies
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Ensure Date is datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Feature extraction from Date
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)

# Handle categorical variables
if 'PromoInterval' in df.columns:
    df['PromoInterval'] = df['PromoInterval'].fillna("None")
    df = pd.get_dummies(df, columns=['PromoInterval'], drop_first=True)

if 'StoreType' in df.columns:
    df = pd.get_dummies(df, columns=['StoreType'], drop_first=True)

if 'Assortment' in df.columns:
    df = pd.get_dummies(df, columns=['Assortment'], drop_first=True)

# Drop original Date column
df = df.drop(columns=['Date'], errors='ignore')

# split data into target variable and others
X = df.drop(["Sales"], axis=1)
y = df["Sales"]

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestRegressor(n_estimators=50, random_state=42))
])

#prediction time
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

# Evaluate
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(rmse)

"""why we use RMSE
It’s easy to implement,
It penalizes big sales prediction mistakes,
It’s in the same unit as sales (so business people understand it).

2.4 Post Prediction analysis
"""

# first we will think importance of our feature
# Extract feature importances
importances = pipeline.named_steps["model"].feature_importances_
features = X_train.columns

# Create DataFrame
feat_imp = pd.DataFrame({"Feature": features, "Importance": importances})
feat_imp = feat_imp.sort_values(by="Importance", ascending=False)

# Plot
plt.figure(figsize=(10,5))
plt.barh(feat_imp["Feature"], feat_imp["Importance"])
plt.gca().invert_yaxis()
plt.title("Feature Importances (RandomForest)")
plt.show()

"""2.5 Serialize models

"""

import joblib
from datetime import datetime

# Get current timestamp
timestamp = datetime.now().strftime("%d-%m-%Y-%H-%M-%S-00")

# Fit pipeline properly
pipeline.fit(X_train, y_train)

# Now save only the pipeline (which contains fitted scaler + model)
import joblib
from datetime import datetime

timestamp = datetime.now().strftime("%d-%m-%Y-%H-%M-%S-00")
filename = f"model_{timestamp}.pkl"

joblib.dump(pipeline, filename, compress=3)

# Load the model
loaded_model = joblib.load(filename)

# Use for prediction
y_pred = loaded_model.predict(X_test)
print(y_pred)

"""2.6 Building model with deep learning

"""

# first inport all neccessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# 1: Load and Isolate Time Series
# Suppose train.csv has "Date" and "Sales"
df = pd.read_csv("train.csv", parse_dates=["Date"])
df = df.sort_values("Date")

# Use store 1 for simplicity
store_sales = df[df["Store"] == 1].set_index("Date")["Sales"]

# visualisation
plt.figure(figsize=(12,4))
plt.plot(store_sales)
plt.title("Store 1 Sales Over Time")
plt.show()

# 4: ACF & PACF
# ----------------------------
fig, ax = plt.subplots(1,2, figsize=(12,4))
plot_acf(store_sales, lags=30, ax=ax[0])
plot_pacf(store_sales, lags=30, ax=ax[1])
plt.show()

# 5: Sliding Window to Supervised Data
# ----------------------------
def create_sliding_window(data, window_size=30):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# 6: Scaling
# ----------------------------
scaler = MinMaxScaler(feature_range=(-1,1))
sales_scaled = scaler.fit_transform(store_sales.values.reshape(-1,1))

window_size = 30
X, y = create_sliding_window(sales_scaled, window_size)

# Reshape for LSTM: [samples, timesteps, features]
X = X.reshape((X.shape[0], X.shape[1], 1))

# Train/Test Split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# 7: Build 2-layer LSTM
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(window_size,1)))
model.add(Dropout(0.2))
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer="adam", loss="mse")
model.summary()

# Train
history = model.fit(X_train, y_train, epochs=20, batch_size=32,
                    validation_data=(X_test, y_test), verbose=1)

# Step 8: Predictions
# ----------------------------
y_pred = model.predict(X_test)
y_pred = scaler.inverse_transform(y_pred)  # invert scaling
y_test_inv = scaler.inverse_transform(y_test.reshape(-1,1))

# visualisation
plt.figure(figsize=(12,4))
plt.plot(y_test_inv, label="True Sales")
plt.plot(y_pred, label="Predicted Sales")
plt.legend()
plt.show()



"""2.7 Using MLFlow to serve the prediction"""

!pip install mlflow

# first import all neccessary libraries
import mlflow
import mlflow.sklearn
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load data
X, y = load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = RandomForestRegressor(n_estimators=10, random_state=42)
model.fit(X_train, y_train)

# Log with MLflow
with mlflow.start_run() as run:
    mlflow.sklearn.log_model(model, "model")
    mlflow.log_metric("mse", mean_squared_error(y_test, model.predict(X_test)))
    run_id = run.info.run_id

print("Run ID:", run_id)

!pip install pyngrok

from pyngrok import ngrok
from getpass import getpass

# Set your ngrok authtoken (replace with your actual token or use getpass)
# ngrok.set_auth_token("YOUR_NGROK_AUTH_TOKEN")
# Or use getpass to securely enter your token
ngrok.set_auth_token(getpass("Enter your ngrok authtoken: "))


# Start MLflow server
run_id_to_serve = "65e8d19837e041478b286cb1a422fc0e" # Use a valid variable name
get_ipython().system_raw(f"mlflow models serve -m runs:/{run_id_to_serve}/model -p 5000 --no-conda &")

# Expose port 5000 with ngrok
public_url = ngrok.connect(5000)
print("MLflow Model Serving URL:", public_url)